# Introduction
This project sought to build a scehduling data ppipeline to load from S3 Bucket to modern data warehouse platform Snowflake.
To ochestrate such a data pipeline between S3 and Snowflake, Apache Airflow an open-source workflow management 
platform is used to author and manage this pipleine while installed on Docker.



# Dataset
The datasets used for this project is here [Tickets_Data](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbjBTU1lBTWl0VExvVHg1aUxzNUFZYVRrM2xHUXxBQ3Jtc0tsaExOMGVqYVlRR1FhTjdiQTVuU3o4NWI0RWpUQXBZNlRWYUJpTmh1SElqeGlhYmtPaUJ5UHNBX1dDYXFhSVRkVFRTRUdqcFNEM0xHQlp0anFFWW00bjJ3REtjSGZpRmdqeHhtNTNYMDZqU2p4RjVkSQ&q=https%3A%2F%2Fdocs.aws.amazon.com%2Fredshift%2Flatest%2Fgsg%2Fsamples%2Ftickitdb.zip&v=BopMJPEH6AE) courtesy of Darshil Parmer, a [Youtuber](https://www.youtube.com/@DarshilParmar) and freelance Data Engineer.



# Tools




# ETL Batch Processing




# Conclusion
